{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Loan Approval Model - Notebook-Friendly Single-File Script (Patched, Protected-Feature Handling)\n",
    "\n",
    "- No argparse; main() is parameter-driven.\n",
    "- Protected/Derived/Given handling via feature_classification.csv:\n",
    "    * Only \"Derived\" + \"Given\" features are used for training.\n",
    "    * All \"Protected\" features are excluded and logged.\n",
    "    * Saves a feature_tags.csv showing tag per feature and whether used.\n",
    "- Robust preprocessing pipeline:\n",
    "    * Median imputation (numeric), most_frequent (categorical)\n",
    "    * Outlier capping per-feature: Z-score (≈normal) else IQR-based\n",
    "    * RobustScaler (median/IQR scaling)\n",
    "- Dynamic, safe feature selection to avoid KeyError.\n",
    "- GridSearchCV with compatible solver/penalty combinations.\n",
    "- StratifiedKFold with safe n_splits based on minority class size.\n",
    "- Robust target casting to avoid IntCastingNaNError.\n",
    "- Returns artifacts & model; also saves JSON + pickle to out_dir.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score, roc_auc_score,\n",
    "                             average_precision_score, classification_report)\n",
    "\n",
    "# -----------------------------\n",
    "# Global config (edit these variables)\n",
    "# -----------------------------\n",
    "\n",
    "DEFAULT_DATA = \"Data/Loan_dataset_india_20000.csv\"\n",
    "DEFAULT_TARGET = \"credit_approved\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Path to the feature classification file (columns: Feature, Classification)\n",
    "# Values in Classification should be one of: Protected, Derived, Given (case-insensitive).\n",
    "FEATURE_CLASSIFICATION: Optional[str] = \"data/feature_classification.csv\"\n",
    "\n",
    "# I/O and training knobs\n",
    "OUT_DIR: str = \"Data\"\n",
    "TEST_SIZE: float = 0.3\n",
    "CV_FOLDS: int = 1\n",
    "SMOKE: bool = False\n",
    "SAMPLE_NROWS: Optional[int] = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def make_ohe():\n",
    "    \"\"\"Return OneHotEncoder compatible with sklearn>=1.2 (sparse_output) and older versions (sparse).\"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "\n",
    "def safe_intersection(names: List[str], cols: List[str]) -> List[str]:\n",
    "    \"\"\"Return items of names that exist in cols, preserving order.\"\"\"\n",
    "    colset = set(cols)\n",
    "    return [n for n in names if n in colset]\n",
    "\n",
    "\n",
    "def read_feature_classification(csv_path: Optional[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Read feature classification CSV if provided and exists; otherwise return None.\"\"\"\n",
    "    if not csv_path:\n",
    "        return None\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists():\n",
    "        print(f\"[warn] feature_classification not found at: {csv_path}. Proceeding without tags.\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        # Normalize names\n",
    "        colmap = {c.lower(): c for c in df.columns}\n",
    "        feat_col = colmap.get(\"feature\", \"Feature\" if \"Feature\" in df.columns else df.columns[0])\n",
    "        cls_col = colmap.get(\"classification\", \"Classification\" if \"Classification\" in df.columns else df.columns[1])\n",
    "        df = df.rename(columns={feat_col: \"Feature\", cls_col: \"Classification\"})\n",
    "        df[\"Feature\"] = df[\"Feature\"].astype(str)\n",
    "        df[\"Classification\"] = (df[\"Classification\"].astype(str)\n",
    "                                .str.strip().str.lower()\n",
    "                                .map({\"protected\": \"protected\", \"derived\": \"derived\", \"given\": \"given\"})\n",
    "                                .fillna(\"given\"))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Failed to read/parse feature_classification '{csv_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def tag_features_from_classification(all_cols: List[str], fc_df: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a tag dataframe with columns: feature, tag, raw_tag.\n",
    "    If fc_df is None or feature missing, default to 'given'.\n",
    "    \"\"\"\n",
    "    tag_map = {}\n",
    "    if fc_df is not None:\n",
    "        tag_map = dict(zip(fc_df[\"Feature\"].astype(str), fc_df[\"Classification\"]))\n",
    "\n",
    "    rows = []\n",
    "    for c in all_cols:\n",
    "        t = tag_map.get(c, \"given\")\n",
    "        tag = t if t in {\"given\", \"derived\", \"protected\"} else \"given\"\n",
    "        rows.append({\"feature\": c, \"tag\": tag, \"raw_tag\": t})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def infer_feature_types(df: pd.DataFrame,\n",
    "                        target: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Infer numerical and categorical features from df (excluding target).\"\"\"\n",
    "    cols = [c for c in df.columns if c != target]\n",
    "    numeric_cols = df[cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in cols if c not in numeric_cols]\n",
    "    return numeric_cols, cat_cols\n",
    "\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Caps numeric features column-wise using either:\n",
    "        - Z-score (mean ± 3*std) if abs(skew) < 1 (roughly normal), else\n",
    "        - IQR method (Q1 - 1.5*IQR, Q3 + 1.5*IQR) for skewed distributions.\n",
    "    Thresholds are learned on training data and applied to transform.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.bounds_: List[Tuple[Optional[float], Optional[float]]] = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_arr = self._to_array(X)\n",
    "        self.bounds_ = []\n",
    "        for i in range(X_arr.shape[1]):\n",
    "            col = X_arr[:, i]\n",
    "            col_nonan = col[~np.isnan(col)]\n",
    "            if col_nonan.size == 0:\n",
    "                self.bounds_.append((None, None))\n",
    "                continue\n",
    "            skewness = pd.Series(col_nonan).skew()\n",
    "            if abs(skewness) < 1:\n",
    "                m, s = float(np.mean(col_nonan)), float(np.std(col_nonan, ddof=0))\n",
    "                if s == 0 or np.isnan(s):\n",
    "                    low, high = m, m\n",
    "                else:\n",
    "                    low, high = m - 3*s, m + 3*s\n",
    "            else:\n",
    "                Q1, Q3 = np.percentile(col_nonan, 25), np.percentile(col_nonan, 75)\n",
    "                IQR = Q3 - Q1\n",
    "                if IQR == 0:\n",
    "                    low, high = Q1, Q3\n",
    "                else:\n",
    "                    low, high = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "            self.bounds_.append((low, high))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_arr = self._to_array(X)\n",
    "        X_capped = X_arr.copy()\n",
    "        for i, (low, high) in enumerate(self.bounds_):\n",
    "            if low is not None and high is not None:\n",
    "                X_capped[:, i] = np.clip(X_arr[:, i], low, high)\n",
    "        return X_capped\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_array(X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.values.astype(float)\n",
    "        return np.asarray(X, dtype=float)\n",
    "\n",
    "\n",
    "def build_preprocessor(numeric_cols: List[str], cat_cols: List[str]) -> ColumnTransformer:\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('outlier', OutlierCapper()),\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', make_ohe())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def build_pipeline(preprocessor: ColumnTransformer) -> Pipeline:\n",
    "    clf = LogisticRegression(random_state=RANDOM_STATE)\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def get_param_grid(smoke: bool = False) -> List[Dict[str, Any]]:\n",
    "    if smoke:\n",
    "        return [\n",
    "            {\n",
    "                'clf__solver': ['lbfgs'],\n",
    "                'clf__penalty': ['l2'],\n",
    "                'clf__C': [1.0],\n",
    "                'clf__class_weight': [None, 'balanced'],\n",
    "                'clf__max_iter': [200]\n",
    "            }\n",
    "        ]\n",
    "    return [\n",
    "        {\n",
    "            'clf__solver': ['liblinear'],\n",
    "            'clf__penalty': ['l1', 'l2'],\n",
    "            'clf__C': [0.1, 1.0, 10.0],\n",
    "            'clf__class_weight': [None, 'balanced'],\n",
    "            'clf__max_iter': [200]\n",
    "        },\n",
    "        {\n",
    "            'clf__solver': ['lbfgs'],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__C': [0.1, 1.0, 10.0],\n",
    "            'clf__class_weight': [None, 'balanced'],\n",
    "            'clf__max_iter': [200]\n",
    "        },\n",
    "        {\n",
    "            'clf__solver': ['saga'],\n",
    "            'clf__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'clf__l1_ratio': [0.0, 0.5, 1.0],\n",
    "            'clf__C': [0.1, 1.0, 10.0],\n",
    "            'clf__class_weight': [None, 'balanced'],\n",
    "            'clf__max_iter': [500]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred, y_proba=None) -> Dict[str, Any]:\n",
    "    metrics = {\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "        except Exception:\n",
    "            metrics['roc_auc'] = None\n",
    "        try:\n",
    "            metrics['pr_auc'] = average_precision_score(y_true, y_proba)\n",
    "        except Exception:\n",
    "            metrics['pr_auc'] = None\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def apply_feature_tags_and_filter(df: pd.DataFrame,\n",
    "                                  target: str,\n",
    "                                  fc_path: Optional[str]) -> Tuple[pd.DataFrame, pd.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Returns: (df_filtered, tag_df, allowed_features, protected_excluded)\n",
    "    - tag_df columns: feature, tag, raw_tag, used_in_training (bool)\n",
    "    \"\"\"\n",
    "    all_cols = [c for c in df.columns if c != target]\n",
    "    fc_df = read_feature_classification(fc_path)\n",
    "    tags = tag_features_from_classification(all_cols, fc_df)\n",
    "    allowed = tags[tags['tag'].isin(['given', 'derived'])]['feature'].tolist()\n",
    "    protected_excluded = tags[tags['tag'] == 'protected']['feature'].tolist()\n",
    "    # Filter df to allowed features + target\n",
    "    df_filtered = pd.concat([df[allowed], df[target]], axis=1)\n",
    "    tags['used_in_training'] = tags['feature'].isin(allowed)\n",
    "    return df_filtered, tags, allowed, protected_excluded\n",
    "\n",
    "\n",
    "def run_training(df: pd.DataFrame,\n",
    "                 target: str,\n",
    "                 cv_folds: int = 5,\n",
    "                 test_size: float = 0.2,\n",
    "                 random_state: int = RANDOM_STATE,\n",
    "                 metadata: Optional[str] = None,\n",
    "                 smoke: bool = False,\n",
    "                 feature_classification: Optional[str] = FEATURE_CLASSIFICATION,\n",
    "                 out_dir: str = OUT_DIR) -> Dict[str, Any]:\n",
    "    assert target in df.columns, f\"Target column '{target}' not found in data.\"\n",
    "\n",
    "    # Drop any clearly ID-like columns if present (non-informative)\n",
    "    drop_like = [c for c in df.columns if c.lower() in {'loan_id', 'id', 'application_id'}]\n",
    "    if drop_like:\n",
    "        df = df.drop(columns=drop_like)\n",
    "\n",
    "    # --- Robust target handling to avoid IntCastingNaNError ---\n",
    "    y_raw = df[target]\n",
    "    mapping = {\n",
    "        'y': 1, 'yes': 1, 'true': 1, 't': 1, 'approved': 1, 'approve': 1, '1': 1,\n",
    "        'n': 0, 'no': 0, 'false': 0, 'f': 0, 'rejected': 0, 'reject': 0, '0': 0\n",
    "    }\n",
    "    y_mapped = None\n",
    "    if not pd.api.types.is_numeric_dtype(y_raw):\n",
    "        y_lower = y_raw.astype(str).str.strip().str.lower()\n",
    "        y_mapped = y_lower.map(mapping)\n",
    "        if y_mapped.isna().mean() > 0.1:\n",
    "            y_mapped = None\n",
    "\n",
    "    if y_mapped is not None:\n",
    "        y_series = y_mapped\n",
    "    else:\n",
    "        y_num = pd.to_numeric(y_raw, errors='coerce')\n",
    "        y_series = y_num\n",
    "\n",
    "    # Drop rows with missing target\n",
    "    mask_valid = y_series.notna()\n",
    "    if mask_valid.sum() < len(y_series):\n",
    "        print(f\"[warn] Dropping {len(y_series) - mask_valid.sum()} rows with missing target '{target}'.\")\n",
    "    df = df.loc[mask_valid].copy()\n",
    "    y_series = y_series.loc[mask_valid]\n",
    "\n",
    "    # If series is float but actually binary like {0.0,1.0}, cast to int\n",
    "    uniques = pd.Series(y_series.dropna().unique())\n",
    "    if set(uniques.tolist()).issubset({0, 1, 0.0, 1.0}):\n",
    "        y = y_series.astype(int)\n",
    "    else:\n",
    "        if not pd.api.types.is_integer_dtype(y_series):\n",
    "            y = y_series.astype('category').cat.codes\n",
    "        else:\n",
    "            y = y_series\n",
    "\n",
    "    n_classes = pd.Series(y).nunique()\n",
    "    if n_classes < 2:\n",
    "        raise ValueError(f\"Target '{target}' has only one class after cleaning; cannot train.\")\n",
    "\n",
    "    # --- Apply feature tags & filter protected ---\n",
    "    df_tagged, tag_df, allowed_features, protected_excluded = apply_feature_tags_and_filter(\n",
    "        df=df, target=target, fc_path=feature_classification\n",
    "    )\n",
    "\n",
    "    # Train/test split\n",
    "    X = df_tagged.drop(columns=[target])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Infer features from TRAIN ONLY to avoid leakage and to ensure presence\n",
    "    num_cols, cat_cols = infer_feature_types(X_train, target=\"\")\n",
    "\n",
    "    if not num_cols and not cat_cols:\n",
    "        num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "    preprocessor = build_preprocessor(num_cols, cat_cols)\n",
    "    pipe = build_pipeline(preprocessor)\n",
    "    param_grid = get_param_grid(smoke=smoke)\n",
    "\n",
    "    # Safe StratifiedKFold\n",
    "    min_class_count = int(pd.Series(y_train).value_counts().min())\n",
    "    n_splits = max(2, min(int(cv_folds), min_class_count))\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    print(\"[info] Starting GridSearchCV ...\")\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best = grid.best_estimator_\n",
    "    y_pred = best.predict(X_test)\n",
    "    y_proba = None\n",
    "    if hasattr(best, \"predict_proba\"):\n",
    "        try:\n",
    "            y_proba = best.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "\n",
    "    metrics = evaluate(y_test, y_pred, y_proba=y_proba)\n",
    "    print(\"\\n[report] Classification report on test:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Save feature tags alongside artifacts\n",
    "    out_dir_path = Path(out_dir)\n",
    "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    tag_path = out_dir_path / \"feature_tags.csv\"\n",
    "    tag_df.to_csv(tag_path, index=False)\n",
    "\n",
    "    artifacts = {\n",
    "        'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'target': target,\n",
    "        'numeric_features': num_cols,\n",
    "        'categorical_features': cat_cols,\n",
    "        'allowed_features': allowed_features,\n",
    "        'protected_features_excluded': protected_excluded,\n",
    "        'feature_tags_csv': str(tag_path),\n",
    "        'best_params': grid.best_params_,\n",
    "        'best_score_mean_cv_f1': float(grid.best_score_),\n",
    "        'test_metrics': metrics\n",
    "    }\n",
    "    return artifacts, best\n",
    "\n",
    "\n",
    "def main(data_path: str = DEFAULT_DATA,\n",
    "         target: str = DEFAULT_TARGET,\n",
    "         metadata: Optional[str] = None,\n",
    "         test_size: float = TEST_SIZE,\n",
    "         cv_folds: int = CV_FOLDS,\n",
    "         random_state: int = RANDOM_STATE,\n",
    "         smoke: bool = SMOKE,\n",
    "         sample_nrows: Optional[int] = SAMPLE_NROWS,\n",
    "         out_dir: str = OUT_DIR,\n",
    "         feature_classification: Optional[str] = FEATURE_CLASSIFICATION):\n",
    "    \"\"\"\n",
    "    Notebook-friendly main(). Pass parameters directly.\n",
    "    Returns (artifacts: dict, model: estimator).\n",
    "    \"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    assert data_path.exists(), f\"Data file not found: {data_path}\"\n",
    "    print(f\"[info] Loading data: {data_path}\")\n",
    "\n",
    "    read_kwargs: Dict[str, Any] = {}\n",
    "    if smoke and sample_nrows is None:\n",
    "        read_kwargs['nrows'] = 2000  # small sample for quicker run\n",
    "    elif sample_nrows is not None:\n",
    "        read_kwargs['nrows'] = int(sample_nrows)\n",
    "\n",
    "    df = pd.read_csv(data_path, **read_kwargs)\n",
    "\n",
    "    # Basic cleaning: strip column names\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    # If target not found, try common fallbacks\n",
    "    if target not in df.columns:\n",
    "        candidates = [c for c in df.columns if c.lower() in {\"credit_approved\", \"approved\", \"label\", \"target\"}]\n",
    "        if candidates:\n",
    "            print(f\"[warn] Target '{target}' not found. Using '{candidates[0]}' instead.\")\n",
    "            target = candidates[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Target column '{target}' not found and no fallback detected.\")\n",
    "\n",
    "    artifacts, model = run_training(\n",
    "        df=df,\n",
    "        target=target,\n",
    "        cv_folds=cv_folds,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        metadata=metadata if metadata else None,\n",
    "        smoke=smoke,\n",
    "        feature_classification=feature_classification,\n",
    "        out_dir=out_dir\n",
    "    )\n",
    "\n",
    "    # Save artifacts\n",
    "    out_dir_path = Path(out_dir)\n",
    "    out_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    artifacts_path = out_dir_path / \"run_artifacts.json\"\n",
    "    model_path = out_dir_path / \"best_model.pkl\"\n",
    "\n",
    "    with open(artifacts_path, \"w\") as f:\n",
    "        json.dump(artifacts, f, indent=2)\n",
    "\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n=== Run Summary ===\")\n",
    "    print(json.dumps(artifacts, indent=2))\n",
    "    print(f\"\\nArtifacts saved to: {artifacts_path}\")\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    print(f\"Feature tags saved to: {artifacts.get('feature_tags_csv')}\")\n",
    "\n",
    "    return artifacts, model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call main() using the variables defined at the top of the file\n",
    "    main(\n",
    "        data_path=DEFAULT_DATA,\n",
    "        target=DEFAULT_TARGET,\n",
    "        metadata=None,\n",
    "        test_size=TEST_SIZE,\n",
    "        cv_folds=CV_FOLDS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        smoke=SMOKE,\n",
    "        sample_nrows=SAMPLE_NROWS,\n",
    "        out_dir=OUT_DIR,\n",
    "        feature_classification=FEATURE_CLASSIFICATION\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018bead",
   "metadata": {},
   "source": [
    "## code wiht threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8205f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/6bdr99fj10b8r3lfc1hyq1_40000gn/T/ipykernel_60560/789805228.py:179: DtypeWarning: Columns (0,1,2,9,10,14,15,16,19,26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DF (110000, 29)\n",
      "[warn] dropped 85000 rows with invalid target\n",
      "Dataframe after cleaning: (25000, 29)\n",
      "[info] Effective grid combos: 6  |  CV folds: 5  |  total fits: 30\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "[report] Post-threshold classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.80      0.88      4509\n",
      "           1       0.76      0.97      0.85      2991\n",
      "\n",
      "    accuracy                           0.87      7500\n",
      "   macro avg       0.87      0.88      0.86      7500\n",
      "weighted avg       0.89      0.87      0.87      7500\n",
      "\n",
      "\n",
      "=== Summary ===\n",
      "{\n",
      "  \"best_params\": {\n",
      "    \"clf__C\": 1000,\n",
      "    \"clf__class_weight\": \"balanced\",\n",
      "    \"clf__max_iter\": 50,\n",
      "    \"clf__penalty\": \"l2\",\n",
      "    \"clf__solver\": \"lbfgs\"\n",
      "  },\n",
      "  \"best_score_mean_cv_f1\": 0.8508354167393648,\n",
      "  \"threshold_constraints\": {\n",
      "    \"min_precision\": 0.6,\n",
      "    \"min_recall\": 0.6,\n",
      "    \"tuning\": {\n",
      "      \"threshold\": 0.42297931660877264,\n",
      "      \"precision\": 0.7606635071090048,\n",
      "      \"recall\": 0.9658976930792377,\n",
      "      \"f1\": 0.8510826336716237,\n",
      "      \"constraints_met\": true\n",
      "    }\n",
      "  },\n",
      "  \"test_metrics\": {\n",
      "    \"f1\": 0.8510826336721167,\n",
      "    \"precision\": 0.7606635071090048,\n",
      "    \"recall\": 0.9658976930792377,\n",
      "    \"roc_auc\": 0.9059002986634184,\n",
      "    \"pr_auc\": 0.7990776947168582\n",
      "  },\n",
      "  \"protected_features_excluded\": [\n",
      "    \"education_level\",\n",
      "    \"person_age\",\n",
      "    \"person_gender\",\n",
      "    \"marital_status\",\n",
      "    \"geographic_region\"\n",
      "  ],\n",
      "  \"allowed_features_count\": 23,\n",
      "  \"solvers\": [\n",
      "    \"lbfgs\",\n",
      "    \"saga\"\n",
      "  ],\n",
      "  \"C_grid\": [\n",
      "    10,\n",
      "    100,\n",
      "    1000\n",
      "  ],\n",
      "  \"max_iter\": 50,\n",
      "  \"cv_folds\": 5,\n",
      "  \"fast_mode\": false\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x105fd5bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104c95bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1043d1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104779bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10312dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x103fd1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x102595bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x102c69bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104919bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1058edbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Loan Approval Model - L2 Balanced Fast Grid + Threshold Constraints\n",
    "\n",
    "- Only L2 penalty\n",
    "- class_weight='balanced' (fixed)\n",
    "- Narrow, fast grid (lbfgs/saga × C in {0.01, 0.1, 1.0, 10.0})\n",
    "- Higher max_iter for convergence\n",
    "- Threshold tuning to require precision >= MIN_PRECISION and recall >= MIN_RECALL (if achievable)\n",
    "\"\"\"\n",
    "\n",
    "import json, pickle, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score, roc_auc_score,\n",
    "                             average_precision_score, classification_report, precision_recall_curve)\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DEFAULT_DATA = \"Data/Loan_dataset_india_110000_final_update copy(in).csv\"\n",
    "DEFAULT_TARGET = \"credit_approved\"\n",
    "FEATURE_CLASSIFICATION: Optional[str] = \"data/feature_classification.csv\"\n",
    "OUT_DIR = \"Data\"\n",
    "\n",
    "# speed/quality knobs\n",
    "CV_FOLDS = 5  # faster\n",
    "FAST_MODE = False        # if True -> only lbfgs (4 combos). If False -> lbfgs & saga (8 combos)\n",
    "MAX_ITER = 50  # higher convergence budget\n",
    "C_GRID = [10, 100, 1000]\n",
    "\n",
    "# threshold constraints\n",
    "MIN_PRECISION = 0.60\n",
    "MIN_RECALL = 0.60\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3\n",
    "SMOKE = False\n",
    "SAMPLE_NROWS: Optional[int] = None\n",
    "\n",
    "# --------------- Utils ------------------\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=0.01)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def read_feature_classification(csv_path: Optional[str]) -> Optional[pd.DataFrame]:\n",
    "    if not csv_path: return None\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists(): return None\n",
    "    df = pd.read_csv(p)\n",
    "    colmap = {c.lower(): c for c in df.columns}\n",
    "    feat = colmap.get(\"feature\", list(df.columns)[0])\n",
    "    cls  = colmap.get(\"classification\", list(df.columns)[1])\n",
    "    df = df.rename(columns={feat:\"Feature\", cls:\"Classification\"})\n",
    "    df[\"Feature\"] = df[\"Feature\"].astype(str)\n",
    "    df[\"Classification\"] = (df[\"Classification\"].astype(str).str.strip().str.lower()\n",
    "                            .map({\"protected\":\"protected\",\"derived\":\"derived\",\"given\":\"given\"}).fillna(\"given\"))\n",
    "    return df\n",
    "\n",
    "def tag_features_from_classification(all_cols: List[str], fc_df: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    tag_map = {} if fc_df is None else dict(zip(fc_df[\"Feature\"].astype(str), fc_df[\"Classification\"]))\n",
    "    rows = []\n",
    "    for c in all_cols:\n",
    "        t = tag_map.get(c, \"given\")\n",
    "        tag = t if t in {\"given\",\"derived\",\"protected\"} else \"given\"\n",
    "        rows.append({\"feature\": c, \"tag\": tag, \"raw_tag\": t})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def apply_feature_tags_and_filter(df: pd.DataFrame, target: str, fc_path: Optional[str]):\n",
    "    cols = [c for c in df.columns if c != target]\n",
    "    tags = tag_features_from_classification(cols, read_feature_classification(fc_path))\n",
    "    allowed = tags[tags[\"tag\"].isin([\"given\",\"derived\"])][\"feature\"].tolist()\n",
    "    protected = tags[tags[\"tag\"]==\"protected\"][\"feature\"].tolist()\n",
    "    out = pd.concat([df[allowed], df[target]], axis=1)\n",
    "    tags[\"used_in_training\"] = tags[\"feature\"].isin(allowed)\n",
    "    return out, tags, allowed, protected\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): self.bounds_ = []\n",
    "    def fit(self, X, y=None):\n",
    "        A = self._arr(X); self.bounds_ = []\n",
    "        for i in range(A.shape[1]):\n",
    "            col = A[:, i]; mask = ~np.isnan(col); v = col[mask]\n",
    "            if v.size == 0: self.bounds_.append((None,None)); continue\n",
    "            skew = pd.Series(v).skew()\n",
    "            if abs(skew) < 1:\n",
    "                m, s = float(np.mean(v)), float(np.std(v, ddof=0))\n",
    "                low, high = (m, m) if (s==0 or np.isnan(s)) else (m-3*s, m+3*s)\n",
    "            else:\n",
    "                q1, q3 = np.percentile(v,25), np.percentile(v,75); iqr = q3-q1\n",
    "                low, high = (q1,q3) if iqr==0 else (q1-1.5*iqr, q3+1.5*iqr)\n",
    "            self.bounds_.append((low, high))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        A = self._arr(X); B = A.copy()\n",
    "        for i,(lo,hi) in enumerate(self.bounds_):\n",
    "            if lo is not None and hi is not None: B[:,i] = np.clip(A[:,i], lo, hi)\n",
    "        return B\n",
    "    @staticmethod\n",
    "    def _arr(X): return X.values.astype(float) if isinstance(X,pd.DataFrame) else np.asarray(X, dtype=float)\n",
    "\n",
    "def infer_feature_types(df: pd.DataFrame, target: str):\n",
    "    cols = [c for c in df.columns if c != target]\n",
    "    num = df[cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat = [c for c in cols if c not in num]\n",
    "    return num, cat\n",
    "\n",
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    num_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                       (\"cap\", OutlierCapper()),\n",
    "                       (\"scale\", RobustScaler())])\n",
    "    cat_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                       (\"ohe\", make_ohe())])\n",
    "    return ColumnTransformer([(\"num\", num_tf, num_cols),\n",
    "                              (\"cat\", cat_tf, cat_cols)], remainder=\"drop\")\n",
    "\n",
    "def build_pipeline(prep):\n",
    "    clf = LogisticRegression(penalty=\"l2\",\n",
    "                             class_weight=\"balanced\",\n",
    "                             max_iter=MAX_ITER,\n",
    "                             random_state=RANDOM_STATE)\n",
    "    return Pipeline([(\"prep\", prep), (\"clf\", clf)])\n",
    "\n",
    "def get_param_grid():\n",
    "    solvers = [\"lbfgs\"] if FAST_MODE else [\"lbfgs\",\"saga\"]\n",
    "    return [{\n",
    "        \"clf__solver\": solvers,\n",
    "        \"clf__penalty\": [\"l2\"],\n",
    "        \"clf__C\": C_GRID,\n",
    "        \"clf__class_weight\": [\"balanced\"],\n",
    "        \"clf__max_iter\": [MAX_ITER]\n",
    "    }]\n",
    "\n",
    "def choose_threshold(y_true, y_proba, min_p, min_r):\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    cand = []\n",
    "    for pi, ri, ti in zip(p[:-1], r[:-1], thr):\n",
    "        if pi >= min_p and ri >= min_r:\n",
    "            f1 = 2*pi*ri/(pi+ri+1e-12); cand.append((ti,pi,ri,f1))\n",
    "    if cand:\n",
    "        ti,pi,ri,f1 = max(cand, key=lambda x:x[3])\n",
    "        return {\"threshold\": float(ti), \"precision\": float(pi), \"recall\": float(ri),\n",
    "                \"f1\": float(f1), \"constraints_met\": True}\n",
    "    f1s = [2*pi*ri/(pi+ri+1e-12) for pi,ri in zip(p[:-1], r[:-1])]\n",
    "    bi = int(np.argmax(f1s)) if f1s else 0\n",
    "    bt = float(thr[bi]) if len(thr)>0 else 0.5\n",
    "    return {\"threshold\": bt, \"precision\": float(p[bi]), \"recall\": float(r[bi]),\n",
    "            \"f1\": float(f1s[bi]) if f1s else 0.0, \"constraints_met\": False}\n",
    "\n",
    "def evaluate(y_true, y_pred, y_proba=None):\n",
    "    m = {\"f1\": f1_score(y_true,y_pred,zero_division=0),\n",
    "         \"precision\": precision_score(y_true,y_pred,zero_division=0),\n",
    "         \"recall\": recall_score(y_true,y_pred,zero_division=0)}\n",
    "    if y_proba is not None:\n",
    "        try: m[\"roc_auc\"] = roc_auc_score(y_true,y_proba)\n",
    "        except: m[\"roc_auc\"] = None\n",
    "        try: m[\"pr_auc\"] = average_precision_score(y_true,y_proba)\n",
    "        except: m[\"pr_auc\"] = None\n",
    "    return m\n",
    "\n",
    "def main(data_path=DEFAULT_DATA, target=DEFAULT_TARGET, feature_classification=FEATURE_CLASSIFICATION):\n",
    "    data_path = Path(data_path); assert data_path.exists(), f\"Missing data: {data_path}\"\n",
    "    read_kwargs = {}\n",
    "    # if SMOKE and SAMPLE_NROWS is None: read_kwargs[\"nrows\"]=2000\n",
    "    # if SAMPLE_NROWS is not None: read_kwargs[\"nrows\"]=int(SAMPLE_NROWS)\n",
    "    # df = pd.read_csv(data_path, **read_kwargs)\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"Shape of DF\",df.shape)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    if target not in df.columns: raise ValueError(f\"Target '{target}' not in data.\")\n",
    "\n",
    "    # Target cleaning\n",
    "    y_raw = df[target]\n",
    "    if not pd.api.types.is_numeric_dtype(y_raw):\n",
    "        mapping = {\"y\":1,\"yes\":1,\"true\":1,\"t\":1,\"approved\":1,\"1\":1,\"n\":0,\"no\":0,\"false\":0,\"f\":0,\"rejected\":0,\"0\":0}\n",
    "        y_series = y_raw.astype(str).str.strip().str.lower().map(mapping)\n",
    "    else:\n",
    "        y_series = pd.to_numeric(y_raw, errors=\"coerce\")\n",
    "    mask = y_series.notna()\n",
    "    if mask.sum()<len(y_series): print(f\"[warn] dropped {len(y_series)-mask.sum()} rows with invalid target\")\n",
    "    df = df.loc[mask].copy(); y = y_series.loc[mask].astype(int)\n",
    "    print(\"Dataframe after cleaning:\", df.shape)\n",
    "\n",
    "    df_f, tags, allowed, protected = apply_feature_tags_and_filter(df, target, feature_classification)\n",
    "\n",
    "    X = df_f.drop(columns=[target])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,\n",
    "                                                        random_state=RANDOM_STATE, stratify=y)\n",
    "    num, cat = infer_feature_types(X_train, target=\"\")\n",
    "    prep = build_preprocessor(num, cat)\n",
    "    pipe = build_pipeline(prep)\n",
    "    grid = get_param_grid()\n",
    "\n",
    "    min_class = int(pd.Series(y_train).value_counts().min())\n",
    "    n_splits = max(2, min(CV_FOLDS, min_class))\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    combos = len(grid[0][\"clf__solver\"]) * len(grid[0][\"clf__C\"])\n",
    "    print(f\"[info] Effective grid combos: {combos}  |  CV folds: {n_splits}  |  total fits: {combos*n_splits}\")\n",
    "\n",
    "    gs = GridSearchCV(pipe, param_grid=grid, cv=cv, scoring=\"f1\", n_jobs=-1, verbose=1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best = gs.best_estimator_\n",
    "\n",
    "    y_proba = best.predict_proba(X_test)[:,1] if hasattr(best,\"predict_proba\") else None\n",
    "    y_pred = best.predict(X_test)\n",
    "    tinfo = None\n",
    "    if y_proba is not None:\n",
    "        tinfo = choose_threshold(y_test, y_proba, MIN_PRECISION, MIN_RECALL)\n",
    "        y_pred = (y_proba >= tinfo[\"threshold\"]).astype(int)\n",
    "\n",
    "    mets = evaluate(y_test, y_pred, y_proba)\n",
    "    print(\"\\n[report] Post-threshold classification report\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    out = {\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"best_score_mean_cv_f1\": float(gs.best_score_),\n",
    "        \"threshold_constraints\": {\"min_precision\": MIN_PRECISION, \"min_recall\": MIN_RECALL, \"tuning\": tinfo},\n",
    "        \"test_metrics\": mets,\n",
    "        \"protected_features_excluded\": protected,\n",
    "        \"allowed_features_count\": len(allowed),\n",
    "        \"solvers\": grid[0][\"clf__solver\"],\n",
    "        \"C_grid\": grid[0][\"clf__C\"],\n",
    "        \"max_iter\": MAX_ITER,\n",
    "        \"cv_folds\": n_splits,\n",
    "        \"fast_mode\": FAST_MODE\n",
    "    }\n",
    "\n",
    "    out_dir = Path(OUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (out_dir/\"feature_tags_fast.csv\").write_text(tags.to_csv(index=False))\n",
    "    with open(out_dir/\"run_artifacts_fast.json\",\"w\") as f: json.dump(out, f, indent=2)\n",
    "    with open(out_dir/\"best_model_fast.pkl\",\"wb\") as f: pickle.dump(best, f)\n",
    "\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(json.dumps(out, indent=2))\n",
    "    if tinfo and not tinfo.get(\"constraints_met\", False):\n",
    "        print(f\"\\n[warn] Could not meet precision>={MIN_PRECISION:.2f} & recall>={MIN_RECALL:.2f} at any threshold. \"\n",
    "              f\"Best: P={tinfo['precision']:.3f}, R={tinfo['recall']:.3f}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d487db1",
   "metadata": {},
   "source": [
    "## 4rd Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Loan Approval Model — Precision-First (L2, balanced) — SPARSE & ROBUST\n",
    "----------------------------------------------------------------------\n",
    "- Precision-first CV scorer (maximize precision at recall floor)\n",
    "- Logistic Regression (L2), class_weight='balanced', solver='saga' (sparse-aware)\n",
    "- OneHotEncoder forced to sparse to avoid memory blow-ups\n",
    "- Optional SelectKBest(mutual_info) kept; can be disabled via search\n",
    "- RandomizedSearchCV over broad C/tol/fit_intercept + selector k\n",
    "- Pipeline caching enabled\n",
    "\"\"\"\n",
    "\n",
    "import json, pickle, time, os, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score, roc_auc_score,\n",
    "                             average_precision_score, classification_report, precision_recall_curve)\n",
    "\n",
    "# ---------------- USER CONFIG ----------------\n",
    "DEFAULT_DATA = \"Data/Loan_dataset_india_110000.csv\"\n",
    "DEFAULT_TARGET = \"credit_approved\"\n",
    "FEATURE_CLASSIFICATION: Optional[str] = \"data/feature_classification.csv\"\n",
    "OUT_DIR = \"Data\"\n",
    "\n",
    "# Precision-first targets\n",
    "PRECISION_TARGET = 0.60\n",
    "RECALL_FLOOR    = 0.20\n",
    "\n",
    "# Search/fit knobs\n",
    "CV_FOLDS   = 5\n",
    "N_ITER     = 36\n",
    "MAX_ITER   = 5000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE  = 0.10\n",
    "SAMPLE_NROWS: Optional[int] = None\n",
    "\n",
    "# OHE rare-level collapse (kept modest; we rely on sparsity)\n",
    "OHE_MIN_FREQ = None  # set to float like 0.05 to collapse rare categories\n",
    "\n",
    "# -----------------------------------------------------\n",
    "\n",
    "def make_ohe():\n",
    "    # Force sparse output; prefer 'sparse_output' if available (>=1.2), else 'sparse'\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True,\n",
    "                             min_frequency=OHE_MIN_FREQ)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "def read_feature_classification(csv_path: Optional[str]) -> Optional[pd.DataFrame]:\n",
    "    if not csv_path: return None\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists(): return None\n",
    "    df = pd.read_csv(p)\n",
    "    colmap = {c.lower(): c for c in df.columns}\n",
    "    feat = colmap.get(\"feature\", list(df.columns)[0])\n",
    "    cls  = colmap.get(\"classification\", list(df.columns)[1])\n",
    "    df = df.rename(columns={feat:\"Feature\", cls:\"Classification\"})\n",
    "    df[\"Feature\"] = df[\"Feature\"].astype(str)\n",
    "    df[\"Classification\"] = (df[\"Classification\"].astype(str).str.strip().str.lower()\n",
    "                            .map({\"protected\":\"protected\",\"derived\":\"derived\",\"given\":\"given\"}).fillna(\"given\"))\n",
    "    return df\n",
    "\n",
    "def tag_features_from_classification(all_cols: List[str], fc_df: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    tag_map = {} if fc_df is None else dict(zip(fc_df[\"Feature\"].astype(str), fc_df[\"Classification\"]))\n",
    "    rows = []\n",
    "    for c in all_cols:\n",
    "        t = tag_map.get(c, \"given\")\n",
    "        tag = t if t in {\"given\",\"derived\",\"protected\"} else \"given\"\n",
    "        rows.append({\"feature\": c, \"tag\": tag, \"raw_tag\": t})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def apply_feature_tags_and_filter(df: pd.DataFrame, target: str, fc_path: Optional[str]):\n",
    "    cols = [c for c in df.columns if c != target]\n",
    "    tags = tag_features_from_classification(cols, read_feature_classification(fc_path))\n",
    "    allowed = tags[tags[\"tag\"].isin([\"given\",\"derived\"])][\"feature\"].tolist()\n",
    "    protected = tags[tags[\"tag\"]==\"protected\"][\"feature\"].tolist()\n",
    "    out = pd.concat([df[allowed], df[target]], axis=1)\n",
    "    tags[\"used_in_training\"] = tags[\"feature\"].isin(allowed)\n",
    "    return out, tags, allowed, protected\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): self.bounds_ = []\n",
    "    def fit(self, X, y=None):\n",
    "        A = self._arr(X); self.bounds_ = []\n",
    "        for i in range(A.shape[1]):\n",
    "            col = A[:, i]; mask = ~np.isnan(col); v = col[mask]\n",
    "            if v.size == 0: self.bounds_.append((None,None)); continue\n",
    "            skew = pd.Series(v).skew()\n",
    "            if abs(skew) < 1:\n",
    "                m, s = float(np.mean(v)), float(np.std(v, ddof=0))\n",
    "                low, high = (m, m) if (s==0 or np.isnan(s)) else (m-3*s, m+3*s)\n",
    "            else:\n",
    "                q1, q3 = np.percentile(v,25), np.percentile(v,75); iqr = q3-q1\n",
    "                low, high = (q1,q3) if iqr==0 else (q1-1.5*iqr, q3+1.5*iqr)\n",
    "            self.bounds_.append((low, high))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        A = self._arr(X); B = A.copy()\n",
    "        for i,(lo,hi) in enumerate(self.bounds_):\n",
    "            if lo is not None and hi is not None: B[:,i] = np.clip(A[:,i], lo, hi)\n",
    "        return B\n",
    "    @staticmethod\n",
    "    def _arr(X): return X.values.astype(float) if isinstance(X,pd.DataFrame) else np.asarray(X, dtype=float)\n",
    "\n",
    "def infer_feature_types(df: pd.DataFrame, target: str):\n",
    "    cols = [c for c in df.columns if c != target]\n",
    "    num = df[cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat = [c for c in cols if c not in num]\n",
    "    return num, cat\n",
    "\n",
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    num_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                       (\"cap\", OutlierCapper()),\n",
    "                       (\"scale\", RobustScaler())])\n",
    "    cat_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                       (\"ohe\", make_ohe())])\n",
    "    return ColumnTransformer([(\"num\", num_tf, num_cols),\n",
    "                              (\"cat\", cat_tf, cat_cols)], remainder=\"drop\")\n",
    "\n",
    "def build_pipeline(prep, cache_dir=\"cache_precision_sparse\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    clf = LogisticRegression(penalty=\"l2\",\n",
    "                             class_weight=\"balanced\",\n",
    "                             solver=\"saga\",            # ensure sparse support\n",
    "                             max_iter=MAX_ITER,\n",
    "                             random_state=RANDOM_STATE)\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=\"all\")\n",
    "    return Pipeline([(\"prep\", prep), (\"select\", selector), (\"clf\", clf)], memory=cache_dir)\n",
    "\n",
    "def choose_threshold_precision_first(y_true, y_proba, precision_target=0.60, recall_floor=0.20):\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    idx = []\n",
    "    for pi, ri, ti in zip(p[:-1], r[:-1], thr):\n",
    "        if (pi >= precision_target) and (ri >= recall_floor):\n",
    "            idx.append((ti, pi, ri, 2*pi*ri/(pi+ri+1e-12)))\n",
    "    if idx:\n",
    "        ti, pi, ri, f1 = max(idx, key=lambda x: (x[2], x[3]))\n",
    "        return {\"threshold\": float(ti), \"precision\": float(pi), \"recall\": float(ri),\n",
    "                \"f1\": float(f1), \"constraints_met\": True, \"criterion\": \"precision>=target & recall>=floor (max recall)\"}\n",
    "    cand = [(ti, pi, ri) for pi, ri, ti in zip(p[:-1], r[:-1], thr) if ri >= recall_floor]\n",
    "    if cand:\n",
    "        ti, pi, ri = max(cand, key=lambda x: x[1])\n",
    "        f1 = 2*pi*ri/(pi+ri+1e-12)\n",
    "        return {\"threshold\": float(ti), \"precision\": float(pi), \"recall\": float(ri),\n",
    "                \"f1\": float(f1), \"constraints_met\": False, \"criterion\": \"max precision @ recall>=floor\"}\n",
    "    f1s = [2*pi*ri/(pi+ri+1e-12) for pi,ri in zip(p[:-1], r[:-1])]\n",
    "    bi = int(np.argmax(f1s)) if f1s else 0\n",
    "    bt = float(thr[bi]) if len(thr)>0 else 0.5\n",
    "    return {\"threshold\": bt, \"precision\": float(p[bi]), \"recall\": float(r[bi]),\n",
    "            \"f1\": float(f1s[bi]) if f1s else 0.0, \"constraints_met\": False, \"criterion\": \"best F1\"}\n",
    "\n",
    "def precision_at_recall_floor_cv(estimator, X, y):\n",
    "    if not hasattr(estimator, \"predict_proba\"):\n",
    "        y_pred = estimator.predict(X)\n",
    "        return precision_score(y, y_pred, zero_division=0)\n",
    "    proba = estimator.predict_proba(X)[:, 1]\n",
    "    p, r, thr = precision_recall_curve(y, proba)\n",
    "    vals = [pi for pi, ri in zip(p[:-1], r[:-1]) if ri >= RECALL_FLOOR]\n",
    "    if not vals:\n",
    "        y_pred = (proba >= 0.5).astype(int)\n",
    "        return precision_score(y, y_pred, zero_division=0) * 0.25\n",
    "    return float(np.max(vals))\n",
    "\n",
    "def evaluate(y_true, y_pred, y_proba=None):\n",
    "    m = {\"f1\": f1_score(y_true,y_pred,zero_division=0),\n",
    "         \"precision\": precision_score(y_true,y_pred,zero_division=0),\n",
    "         \"recall\": recall_score(y_true,y_pred,zero_division=0)}\n",
    "    if y_proba is not None:\n",
    "        try: m[\"roc_auc\"] = roc_auc_score(y_true,y_proba)\n",
    "        except: m[\"roc_auc\"] = None\n",
    "        try: m[\"pr_auc\"] = average_precision_score(y_true,y_proba)\n",
    "        except: m[\"pr_auc\"] = None\n",
    "    return m\n",
    "\n",
    "def main(data_path=DEFAULT_DATA, target=DEFAULT_TARGET, feature_classification=FEATURE_CLASSIFICATION):\n",
    "    data_path = Path(data_path); assert data_path.exists(), f\"Missing data: {data_path}\"\n",
    "    df = pd.read_csv(data_path, nrows=SAMPLE_NROWS if SAMPLE_NROWS is not None else None)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    if target not in df.columns: raise ValueError(f\"Target '{target}' not in data.\")\n",
    "    print(\"[info] shape:\", df.shape)\n",
    "\n",
    "    y_raw = df[target]\n",
    "    if not pd.api.types.is_numeric_dtype(y_raw):\n",
    "        mapping = {\"y\":1,\"yes\":1,\"true\":1,\"t\":1,\"approved\":1,\"1\":1,\"n\":0,\"no\":0,\"false\":0,\"f\":0,\"rejected\":0,\"0\":0}\n",
    "        y_series = y_raw.astype(str).str.strip().str.lower().map(mapping)\n",
    "    else:\n",
    "        y_series = pd.to_numeric(y_raw, errors=\"coerce\")\n",
    "    mask = y_series.notna()\n",
    "    if mask.sum() < len(y_series): print(f\"[warn] dropped {len(y_series)-mask.sum()} rows with invalid target\")\n",
    "    df = df.loc[mask].copy(); y = y_series.loc[mask].astype(int)\n",
    "\n",
    "    df_f, tags, allowed, protected = apply_feature_tags_and_filter(df, target, feature_classification)\n",
    "    X = df_f.drop(columns=[target])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,\n",
    "                                                        random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    num, cat = infer_feature_types(X_train, target=\"\")\n",
    "    prep = build_preprocessor(num, cat)\n",
    "    pipe = build_pipeline(prep, cache_dir=\"cache_precision_sparse\")\n",
    "\n",
    "    # Randomized search space (sparse-friendly)\n",
    "    C_grid = np.logspace(-3, 2, num=40).tolist()\n",
    "    tol_grid = [1e-3, 1e-4, 1e-5]\n",
    "    fit_intercept_grid = [True, False]\n",
    "\n",
    "    param_dist = {\n",
    "        \"clf__C\": C_grid,\n",
    "        \"clf__tol\": tol_grid,\n",
    "        \"clf__fit_intercept\": fit_intercept_grid,\n",
    "        \"select__k\": [\"all\", 200, 400, 800]  # allow disabling by \"all\"\n",
    "    }\n",
    "\n",
    "    min_class = int(pd.Series(y_train).value_counts().min())\n",
    "    n_splits = max(2, min(CV_FOLDS, min_class))\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    print(f\"[info] RandomizedSearchCV: n_iter={N_ITER}, cv={n_splits}, scorer=precision@recall>={RECALL_FLOOR}. (saga + sparse OHE)\")\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=N_ITER,\n",
    "        scoring=precision_at_recall_floor_cv,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    rs.fit(X_train, y_train)\n",
    "    print(f\"[timing] randomized search fit: {time.perf_counter() - t0:.1f}s\")\n",
    "\n",
    "    best = rs.best_estimator_\n",
    "    y_proba = best.predict_proba(X_test)[:,1] if hasattr(best,\"predict_proba\") else None\n",
    "    y_pred = best.predict(X_test)\n",
    "\n",
    "    tinfo = None\n",
    "    if y_proba is not None:\n",
    "        tinfo = choose_threshold_precision_first(\n",
    "            y_test, y_proba,\n",
    "            precision_target=PRECISION_TARGET,\n",
    "            recall_floor=RECALL_FLOOR\n",
    "        )\n",
    "        y_pred = (y_proba >= tinfo[\"threshold\"]).astype(int)\n",
    "\n",
    "    mets = evaluate(y_test, y_pred, y_proba)\n",
    "\n",
    "    out_dir = Path(OUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (out_dir/\"feature_tags_precision_sparse.csv\").write_text(tags.to_csv(index=False))\n",
    "    with open(out_dir/\"best_model_precision_sparse.pkl\",\"wb\") as f: pickle.dump(best, f)\n",
    "\n",
    "    artifacts = {\n",
    "        \"best_params\": rs.best_params_,\n",
    "        \"best_cv_score_precision_at_recall_floor\": float(rs.best_score_),\n",
    "        \"precision_target\": PRECISION_TARGET,\n",
    "        \"recall_floor\": RECALL_FLOOR,\n",
    "        \"threshold_choice\": tinfo,\n",
    "        \"test_metrics\": mets,\n",
    "        \"protected_features_excluded\": protected,\n",
    "        \"allowed_features_count\": len(allowed),\n",
    "        \"max_iter\": MAX_ITER,\n",
    "        \"cv_folds\": n_splits,\n",
    "        \"n_iter\": N_ITER,\n",
    "        \"ohe_min_frequency\": OHE_MIN_FREQ,\n",
    "        \"solver\": \"saga\",\n",
    "        \"sparse_ohe\": True\n",
    "    }\n",
    "    with open(out_dir/\"run_artifacts_precision_sparse.json\",\"w\") as f: json.dump(artifacts, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== Summary (Precision-First SPARSE) ===\")\n",
    "    print(json.dumps(artifacts, indent=2))\n",
    "    if tinfo and not tinfo.get(\"constraints_met\", False):\n",
    "        print(f\"[warn] Could not meet precision>={PRECISION_TARGET:.2f} with recall>={RECALL_FLOOR:.2f}. \"\n",
    "              f\"Used best available: P={tinfo['precision']:.3f}, R={tinfo['recall']:.3f} ({tinfo['criterion']}).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d61b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Loan Approval Model — Precision-First (L2, balanced)\n",
    "----------------------------------------------------\n",
    "- L2 penalty only\n",
    "- class_weight='balanced' (fixed)\n",
    "- Multi-metric CV scoring: precision, recall, f1, PR-AUC, ROC-AUC\n",
    "- Refit rule: maximize mean CV precision subject to mean CV recall >= MIN_RECALL (precision-first with recall floor)\n",
    "- Post-fit threshold tuning: choose threshold that maximizes precision with recall >= MIN_RECALL (if possible)\n",
    "- Expanded solver set (lbfgs, liblinear, newton-cg, saga) + finer C grid to favor higher precision models\n",
    "\"\"\"\n",
    "\n",
    "import json, pickle, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    average_precision_score, classification_report, precision_recall_curve,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DEFAULT_DATA = \"Data/Loan_dataset_india_110000_updated.csv\"\n",
    "DEFAULT_TARGET = \"credit_approved\"\n",
    "FEATURE_CLASSIFICATION: Optional[str] = \"data/feature_classification.csv\"\n",
    "OUT_DIR = \"Data\"\n",
    "\n",
    "# speed/quality knobs\n",
    "CV_FOLDS = 5\n",
    "FAST_MODE = False  # if True -> only lbfgs; else lbfgs/liblinear/newton-cg/saga\n",
    "MAX_ITER = 300\n",
    "\n",
    "# Precision-leaning grid (smaller C often increases conservatism → fewer FPs)\n",
    "C_GRID = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0]\n",
    "\n",
    "# threshold constraints (precision-first with recall floor)\n",
    "MIN_PRECISION = 0.70\n",
    "MIN_RECALL = 0.60\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3\n",
    "SMOKE = False\n",
    "SAMPLE_NROWS: Optional[int] = None\n",
    "\n",
    "# --------------- Utils ------------------\n",
    "def make_ohe():\n",
    "    # Backward compat for older scikit-learn\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=0.01)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def read_feature_classification(csv_path: Optional[str]) -> Optional[pd.DataFrame]:\n",
    "    if not csv_path: return None\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists(): return None\n",
    "    df = pd.read_csv(p)\n",
    "    colmap = {c.lower(): c for c in df.columns}\n",
    "    feat = colmap.get(\"feature\", list(df.columns)[0])\n",
    "    cls  = colmap.get(\"classification\", list(df.columns)[1])\n",
    "    df = df.rename(columns={feat:\"Feature\", cls:\"Classification\"})\n",
    "    df[\"Feature\"] = df[\"Feature\"].astype(str)\n",
    "    df[\"Classification\"] = (\n",
    "        df[\"Classification\"].astype(str).str.strip().str.lower()\n",
    "        .map({\"protected\":\"protected\",\"derived\":\"derived\",\"given\":\"given\"}).fillna(\"given\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def tag_features_from_classification(all_cols: List[str], fc_df: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    tag_map = {} if fc_df is None else dict(zip(fc_df[\"Feature\"].astype(str), fc_df[\"Classification\"]))\n",
    "    rows = []\n",
    "    for c in all_cols:\n",
    "        t = tag_map.get(c, \"given\")\n",
    "        tag = t if t in {\"given\",\"derived\",\"protected\"} else \"given\"\n",
    "        rows.append({\"feature\": c, \"tag\": tag, \"raw_tag\": t})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def apply_feature_tags_and_filter(df: pd.DataFrame, target: str, fc_path: Optional[str]):\n",
    "    cols = [c for c in df.columns if c != target]\n",
    "    tags = tag_features_from_classification(cols, read_feature_classification(fc_path))\n",
    "    allowed = tags[tags[\"tag\"].isin([\"given\",\"derived\"])][\"feature\"].tolist()\n",
    "    protected = tags[tags[\"tag\"]==\"protected\"][\"feature\"].tolist()\n",
    "    out = pd.concat([df[allowed], df[target]], axis=1)\n",
    "    tags[\"used_in_training\"] = tags[\"feature\"].isin(allowed)\n",
    "    return out, tags, allowed, protected\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): self.bounds_ = []\n",
    "    def fit(self, X, y=None):\n",
    "        A = self._arr(X); self.bounds_ = []\n",
    "        for i in range(A.shape[1]):\n",
    "            col = A[:, i]; mask = ~np.isnan(col); v = col[mask]\n",
    "            if v.size == 0: self.bounds_.append((None,None)); continue\n",
    "            skew = pd.Series(v).skew()\n",
    "            if abs(skew) < 1:\n",
    "                m, s = float(np.mean(v)), float(np.std(v, ddof=0))\n",
    "                low, high = (m, m) if (s==0 or np.isnan(s)) else (m-3*s, m+3*s)\n",
    "            else:\n",
    "                q1, q3 = np.percentile(v,25), np.percentile(v,75); iqr = q3-q1\n",
    "                low, high = (q1,q3) if iqr==0 else (q1-1.5*iqr, q3+1.5*iqr)\n",
    "            self.bounds_.append((low, high))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        A = self._arr(X); B = A.copy()\n",
    "        for i,(lo,hi) in enumerate(self.bounds_):\n",
    "            if lo is not None and hi is not None: B[:,i] = np.clip(A[:,i], lo, hi)\n",
    "        return B\n",
    "    @staticmethod\n",
    "    def _arr(X): return X.values.astype(float) if isinstance(X,pd.DataFrame) else np.asarray(X, dtype=float)\n",
    "\n",
    "def infer_feature_types(df: pd.DataFrame, target: str):\n",
    "    cols = [c for c in df.columns if c != target]\n",
    "    num = df[cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat = [c for c in cols if c not in num]\n",
    "    return num, cat\n",
    "\n",
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    num_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                       (\"cap\", OutlierCapper()),\n",
    "                       (\"scale\", RobustScaler())])\n",
    "    cat_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                       (\"ohe\", make_ohe())])\n",
    "    return ColumnTransformer([(\"num\", num_tf, num_cols),\n",
    "                              (\"cat\", cat_tf, cat_cols)], remainder=\"drop\")\n",
    "\n",
    "def build_pipeline(prep):\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=MAX_ITER,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    return Pipeline([(\"prep\", prep), (\"clf\", clf)])\n",
    "\n",
    "def get_param_grid():\n",
    "    solvers = [\"lbfgs\"] if FAST_MODE else [\"lbfgs\", \"liblinear\", \"newton-cg\", \"saga\"]\n",
    "    # liblinear/newton-cg/lbfgs/saga all support L2 for binary LR\n",
    "    return [{\n",
    "        \"clf__solver\": solvers,\n",
    "        \"clf__penalty\": [\"l2\"],\n",
    "        \"clf__C\": C_GRID,\n",
    "        \"clf__class_weight\": [\"balanced\"],\n",
    "        \"clf__max_iter\": [MAX_ITER],\n",
    "        \"clf__tol\": [1e-3, 1e-4],\n",
    "    }]\n",
    "\n",
    "def choose_threshold(y_true, y_proba, min_p, min_r):\n",
    "    \"\"\"Precision-first thresholding with recall floor (maximize precision given recall >= min_r).\"\"\"\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    # candidates that meet recall floor\n",
    "    idxs = [i for i in range(len(thr)) if (p[i] >= min_p and r[i] >= min_r)]\n",
    "    if idxs:\n",
    "        # among feasible thresholds, pick the one with maximum precision; break ties by F1\n",
    "        best_i = max(idxs, key=lambda i: (p[i], (2*p[i]*r[i])/(p[i]+r[i]+1e-12)))\n",
    "        return {\"threshold\": float(thr[best_i]), \"precision\": float(p[best_i]),\n",
    "                \"recall\": float(r[best_i]), \"f1\": float((2*p[best_i]*r[best_i])/(p[best_i]+r[best_i]+1e-12)),\n",
    "                \"constraints_met\": True}\n",
    "    # Fallback: pure precision maximization (may heavily sacrifice recall)\n",
    "    # Use the PR curve points excluding the last (which corresponds to threshold=-inf)\n",
    "    cand_i = int(np.argmax(p[:-1])) if len(p) > 1 else 0\n",
    "    best_thr = float(thr[cand_i]) if len(thr) > 0 else 0.5\n",
    "    return {\"threshold\": best_thr, \"precision\": float(p[cand_i]),\n",
    "            \"recall\": float(r[cand_i]), \"f1\": float((2*p[cand_i]*r[cand_i])/(p[cand_i]+r[cand_i]+1e-12)),\n",
    "            \"constraints_met\": False}\n",
    "\n",
    "def evaluate(y_true, y_pred, y_proba=None):\n",
    "    m = {\"f1\": f1_score(y_true,y_pred,zero_division=0),\n",
    "         \"precision\": precision_score(y_true,y_pred,zero_division=0),\n",
    "         \"recall\": recall_score(y_true,y_pred,zero_division=0)}\n",
    "    if y_proba is not None:\n",
    "        try: m[\"roc_auc\"] = roc_auc_score(y_true,y_proba)\n",
    "        except: m[\"roc_auc\"] = None\n",
    "        try: m[\"pr_auc\"] = average_precision_score(y_true,y_proba)\n",
    "        except: m[\"pr_auc\"] = None\n",
    "    return m\n",
    "\n",
    "def _scorers():\n",
    "    return {\n",
    "        \"precision\": make_scorer(precision_score, zero_division=0),\n",
    "        \"recall\":    make_scorer(recall_score, zero_division=0),\n",
    "        \"f1\":        make_scorer(f1_score, zero_division=0),\n",
    "        \"roc_auc\":   \"roc_auc\",\n",
    "        \"pr_auc\":    \"average_precision\",\n",
    "    }\n",
    "\n",
    "def _refit_precision_with_recall_floor(cv_results):\n",
    "    \"\"\"Select the CV setting with highest mean precision subject to mean recall >= MIN_RECALL.\n",
    "    If none meet the floor, pick the absolute best precision.\"\"\"\n",
    "    prec = np.array(cv_results[\"mean_test_precision\"])\n",
    "    rec  = np.array(cv_results[\"mean_test_recall\"])\n",
    "    feas_idxs = np.where(rec >= MIN_RECALL)[0]\n",
    "    if len(feas_idxs):\n",
    "        return int(feas_idxs[np.argmax(prec[feas_idxs])])\n",
    "    return int(np.argmax(prec))\n",
    "\n",
    "def _topn_cv_by_precision(gs, topn=5):\n",
    "    \"\"\"For reporting: top-N CV rows by mean precision among those meeting recall floor.\"\"\"\n",
    "    df = pd.DataFrame(gs.cv_results_)\n",
    "    if \"mean_test_precision\" not in df or \"mean_test_recall\" not in df: return []\n",
    "    df = df.copy()\n",
    "    df = df[df[\"mean_test_recall\"] >= MIN_RECALL]\n",
    "    if df.empty: \n",
    "        df = pd.DataFrame(gs.cv_results_)  # fallback: ignore recall floor\n",
    "    df = df.sort_values(\"mean_test_precision\", ascending=False).head(topn)\n",
    "    keep_cols = [\n",
    "        \"mean_test_precision\",\"std_test_precision\",\"mean_test_recall\",\"mean_test_f1\",\n",
    "        \"mean_test_pr_auc\",\"mean_test_roc_auc\",\n",
    "        \"param_clf__solver\",\"param_clf__C\",\"param_clf__tol\"\n",
    "    ]\n",
    "    df = df[keep_cols]\n",
    "    # Make JSON-friendly\n",
    "    return json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "def main(data_path=DEFAULT_DATA, target=DEFAULT_TARGET, feature_classification=FEATURE_CLASSIFICATION):\n",
    "    data_path = Path(data_path); assert data_path.exists(), f\"Missing data: {data_path}\"\n",
    "    read_kwargs = {}\n",
    "    # if SMOKE and SAMPLE_NROWS is None: read_kwargs[\"nrows\"]=2000\n",
    "    # if SAMPLE_NROWS is not None: read_kwargs[\"nrows\"]=int(SAMPLE_NROWS)\n",
    "    df = pd.read_csv(data_path, **read_kwargs)\n",
    "    print(\"Shape of DF\", df.shape)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    if target not in df.columns: raise ValueError(f\"Target '{target}' not in data.\")\n",
    "\n",
    "    # Target cleaning\n",
    "    y_raw = df[target]\n",
    "    if not pd.api.types.is_numeric_dtype(y_raw):\n",
    "        mapping = {\"y\":1,\"yes\":1,\"true\":1,\"t\":1,\"approved\":1,\"1\":1,\"n\":0,\"no\":0,\"false\":0,\"f\":0,\"rejected\":0,\"0\":0}\n",
    "        y_series = y_raw.astype(str).str.strip().str.lower().map(mapping)\n",
    "    else:\n",
    "        y_series = pd.to_numeric(y_raw, errors=\"coerce\")\n",
    "    mask = y_series.notna()\n",
    "    if mask.sum()<len(y_series): print(f\"[warn] dropped {len(y_series)-mask.sum()} rows with invalid target\")\n",
    "    df = df.loc[mask].copy(); y = y_series.loc[mask].astype(int)\n",
    "    print(\"Dataframe after cleaning:\", df.shape)\n",
    "\n",
    "    # Exclude protected features\n",
    "    df_f, tags, allowed, protected = apply_feature_tags_and_filter(df, target, feature_classification)\n",
    "\n",
    "    X = df_f.drop(columns=[target])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    num, cat = infer_feature_types(X_train, target=\"\")\n",
    "    prep = build_preprocessor(num, cat)\n",
    "    pipe = build_pipeline(prep)\n",
    "    grid = get_param_grid()\n",
    "\n",
    "    min_class = int(pd.Series(y_train).value_counts().min())\n",
    "    n_splits = max(2, min(CV_FOLDS, min_class))\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    combos = len(grid[0][\"clf__solver\"]) * len(grid[0][\"clf__C\"]) * len(grid[0][\"clf__tol\"])\n",
    "    total_fits = combos * n_splits\n",
    "    print(f\"[info] Effective grid combos: {combos}  |  CV folds: {n_splits}  |  total fits: {total_fits}\")\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid=grid,\n",
    "        cv=cv,\n",
    "        scoring=_scorers(),\n",
    "        refit=_refit_precision_with_recall_floor,  # Precision-first with recall floor\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    best = gs.best_estimator_\n",
    "\n",
    "    # Probability scores & precision-first thresholding\n",
    "    y_proba = best.predict_proba(X_test)[:,1] if hasattr(best,\"predict_proba\") else None\n",
    "    y_pred = best.predict(X_test)\n",
    "    tinfo = None\n",
    "    if y_proba is not None:\n",
    "        tinfo = choose_threshold(y_test, y_proba, MIN_PRECISION, MIN_RECALL)\n",
    "        y_pred = (y_proba >= tinfo[\"threshold\"]).astype(int)\n",
    "\n",
    "    mets = evaluate(y_test, y_pred, y_proba)\n",
    "    print(\"\\n[report] Post-threshold classification report (precision-first)\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    top_cv = _topn_cv_by_precision(gs, topn=5)\n",
    "\n",
    "    out = {\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"best_cv_index\": int(gs.best_index_) if hasattr(gs, \"best_index_\") else None,\n",
    "        \"best_score_mean_cv_precision\": float(gs.cv_results_[\"mean_test_precision\"][gs.best_index_])\n",
    "            if hasattr(gs, \"best_index_\") else None,\n",
    "        \"refit_rule\": \"precision_with_recall_floor\",\n",
    "        \"cv_top5_by_precision\": top_cv,\n",
    "        \"threshold_constraints\": {\n",
    "            \"min_precision\": MIN_PRECISION,\n",
    "            \"min_recall\": MIN_RECALL,\n",
    "            \"tuning\": tinfo\n",
    "        },\n",
    "        \"test_metrics\": mets,\n",
    "        \"protected_features_excluded\": protected,\n",
    "        \"allowed_features_count\": len(allowed),\n",
    "        \"solvers\": grid[0][\"clf__solver\"],\n",
    "        \"C_grid\": grid[0][\"clf__C\"],\n",
    "        \"tol_grid\": grid[0][\"clf__tol\"],\n",
    "        \"max_iter\": MAX_ITER,\n",
    "        \"cv_folds\": n_splits,\n",
    "        \"fast_mode\": FAST_MODE\n",
    "    }\n",
    "\n",
    "    out_dir = Path(OUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (out_dir/\"feature_tags_fast.csv\").write_text(tags.to_csv(index=False))\n",
    "    with open(out_dir/\"run_artifacts_fast.json\",\"w\") as f: json.dump(out, f, indent=2)\n",
    "    with open(out_dir/\"best_model_fast.pkl\",\"wb\") as f: pickle.dump(best, f)\n",
    "\n",
    "    print(\"\\n=== Summary (Precision-first) ===\")\n",
    "    print(json.dumps(out, indent=2))\n",
    "    if tinfo and not tinfo.get(\"constraints_met\", False):\n",
    "        print(f\"\\n[warn] Could not meet precision>={MIN_PRECISION:.2f} & recall>={MIN_RECALL:.2f} at any threshold.\"\n",
    "              f\" Best threshold: P={tinfo['precision']:.3f}, R={tinfo['recall']:.3f}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
